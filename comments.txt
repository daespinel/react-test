#Starts up a development server
npm run start

# Runs tests associated with the project
npm run test

# Builds a production version of the application
npm run build


# In development
docker container: npm run start -> Dockerfile.dev 
docker build -f Dockerfile.dev .

# In production:
docker container: npm run build -> Dockerfile

# a bug 3-22-2020 causes react app to exit after docker run command
(winpty) docker run -it -p 3000:3000 CONTAINER_ID

# Docker volumes: We set up a reference inside the docker container that will
# point back to the local machine

# We have deleted the node_modules folder in tha host machine, because of this, when we do only the second volume attach, we'll have
# a problem related with Docker trying to use the node_modules of the pwd which doesn't exist

# first -v: put a bookmark on the node_modules folder
# second -v: map the pwd into the '/app' folder
docker run -p 3000:3000 -v /app/node_modules -v $(pwd):/app -<image_id>

(in windows) -> Also check if C is shared with containers
 winpty docker run -p 3000:3000 -v /app/node_modules -v /$(PWD):/app 676b2dfadfed
# For some reason, Docker in Windows doesn't understand that it has to take the 
# node_modules from the container but instead it tries to copy all the files from the frontend

# For the docker-compose file, inside build since we have a different Dockerfile, we use 
# build: context: . (to anounce to use the same working directory where we're are) and dockerfile: Dockerfile.dev
# The COPY sentence may be deleted from the dockerfile because in the .yml we mount the volume to the container
# But if in the future we need to change or not to use the docker-compose, it is better to let it in the dockerfile

# Executing tests:
docker build -f Dockerfile.dev . -> docker run <container-id> npm run test

# We need to attach volumes to running containers (even attaching it to in the docker-compose.yml file or in a new service)
# I solution
# (Need to remember container ID and command)
# We can also execute a new command inside the container (changing the test file, then the container updated itself)
docker exec -it <container-id> npm run test

# II solution
# We add a second service in the docker-compose file
#(We don't have the ability to enter the input console to the containers such as re-run tests)
# When we use docker attach, we attach to the stdin of the first running process, in our example npm, for that reason we cannot execute anything with docker attach
docker-compose up --build
docker exec -it <test-container-id>

# npm run build -> builds a production version of the application. We are going to use nginx a simple prod server
# BUILD PHASE
use node:alpine
copy the package.json file
install dependencies (deps only needed to execute 'npm run build')
run 'npm run build'
//start nginx (where's nginx comming?)

# RUN PHASE
use nginx
copy over the result of 'npm run build'
start nginx

# /app/build -> folder of workdir in which we are interesting
docker build . -> sudo docker run -p 8080:80 <container-id>


# Continuous integration and deployment with AWS
# We'll implement a CI/CD chain with github + travis CI + AWS

# In the travis yml:
# Tell travis we need a copy of docker running
# Build our image using Dockerfile.dev
# Tell Travis how to run our tests suite
# Tell travis how to deploy our code to AWS

# We're going to use AWS Elastic Beanstalk -> platform Docker (since our test is for a single container we have to use 64bit Amazon Linux) because in Linux 2 AWS will look for a docker-compose file
# Beanstalk will provision all the necessary AWS elements for our application (load balancer, ec2 instance, security group, auto scaling groups,...)

# We need to create a new user in IAM that will be used by travis ci
# and provide rigths for beanstalk, then we need to use travis ci secrets
# Travis CI > More settings > environment variables
# AWS_access_key and AWS_SECRET_KEY are defined in travis ci

# In Dockerfile we need to explain the exposure of ports for AWS Beanstalk
EXPOSE 80

# Use branches in github > feature branch > pull request to master and deploy
